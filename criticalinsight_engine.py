# -*- coding: utf-8 -*-
"""CriticalInsight_Engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aUju5-AnAh0kfdPRc3iVkd3l5LFqK6pT
"""

# Install required packages
!pip install -q python-docx PyPDF2 pandas openai

# Imports
import os
from openai import OpenAI
from google.colab import files
import pandas as pd
import PyPDF2
import docx
import textwrap

# Put your OpenRouter key here (single key for all models)
os.environ["OPENAI_API_KEY"] = "API_KEY"

# Initialize client (OpenRouter)
client = OpenAI(base_url="https://openrouter.ai/api/v1",
                api_key=os.environ["OPENAI_API_KEY"])

model="google/gemma-2-9b-it"


print("Model set to:", model)

print("Upload a PDF, DOCX, or TXT file (click Choose Files)...")
uploaded = files.upload()
if not uploaded:
    raise SystemExit("No file uploaded.")
filename = list(uploaded.keys())[0]
print("Uploaded:", filename)

def read_file(path):
    text = ""
    if path.lower().endswith(".pdf"):
        reader = PyPDF2.PdfReader(path)
        for p in reader.pages:
            page_txt = p.extract_text()
            if page_txt:
                text += page_txt + "\n\n"
    elif path.lower().endswith(".docx"):
        doc = docx.Document(path)
        for para in doc.paragraphs:
            if para.text and para.text.strip():
                text += para.text.strip() + "\n\n"
    elif path.lower().endswith(".txt"):
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            text = f.read()
    else:
        raise ValueError("Unsupported file type. Use PDF/DOCX/TXT.")
    return text

full_text = read_file(filename)
print("Document length (chars):", len(full_text))

def split_into_clauses(text, min_len=40):
    # split by double newline (paragraphs) then trim small fragments
    paras = [p.strip() for p in text.split("\n\n") if p.strip()]
    clauses = []
    for p in paras:
        if len(p) < min_len:
            # merge to previous if too short
            if clauses:
                clauses[-1] += " " + p
            else:
                clauses.append(p)
        else:
            clauses.append(p)
    return clauses

clauses = split_into_clauses(full_text)
print("Detected clauses/paragraphs:", len(clauses))
# Optionally limit how many you analyze during testing:
#clauses = clauses[:50]

def analyze_clause_text(clause_text, model=model):
    prompt = f"""
You are a precise legal-explainer for everyday users.

Task (strict rules):
- Read the clause below (one paragraph).
- Output EXACTLY three sections only, in this order and no extra text:

1) EXPLANATION: one short sentence in plain English (no legalese).
2) RISK: one short bullet if there is a high or moderate risk (or "None").
3) ACTION: one short instruction the user should take (1 sentence).

Examples (do not print examples in output):
EXPLANATION: Seller must finish work in 30 days.
RISK: Missing definition of "completion" â€” ambiguous.
ACTION: Ask seller to define "completion" and set inspection criteria.

Now analyze the following clause text.

CLAUSE:
\"\"\"{clause_text}\"\"\"
"""
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role":"user","content": prompt}],
            max_tokens=300,
            temperature=0
        )
        out = resp.choices[0].message.content.strip()
        # very small post-clean
        return out
    except Exception as e:
        return f"ERROR: {e}"

# quick test on first clause:
print("\nSample analyze:\n")
print(textwrap.fill(analyze_clause_text(clauses[0]) , width=120))

results = []
for idx, c in enumerate(clauses, start=1):
    # skip empty or heading-like short lines
    if len(c.strip()) < 30:
        continue
    print(f"Analyzing clause {idx}/{len(clauses)} ...", end="\r")
    out = analyze_clause_text(c)
    results.append({"ClauseIndex": idx, "Text": c, "Analysis": out})
print("\nDone analyzing.")

def parse_analysis_block(block):
    # naive parsing expecting labels EXPLANATION:, RISK:, ACTION:
    lines = block.splitlines()
    explanation = risk = action = ""
    cur = None
    for ln in lines:
        ln_stripped = ln.strip()
        if ln_stripped.upper().startswith("EXPLANATION:"):
            cur = "ex"
            explanation = ln_stripped[len("EXPLANATION:"):].strip()
        elif ln_stripped.upper().startswith("RISK:"):
            cur = "rk"
            risk = ln_stripped[len("RISK:"):].strip()
        elif ln_stripped.upper().startswith("ACTION:"):
            cur = "ac"
            action = ln_stripped[len("ACTION:"):].strip()
        else:
            if cur == "ex":
                explanation += " " + ln_stripped
            elif cur == "rk":
                risk += " " + ln_stripped
            elif cur == "ac":
                action += " " + ln_stripped
    # normalization
    if not risk:
        risk = "None"
    return explanation.strip(), risk.strip(), action.strip()

rows = []
for r in results:
    explanation, risk, action = parse_analysis_block(r["Analysis"])
    rows.append({
        "ClauseIndex": r["ClauseIndex"],
        "Explanation": explanation,
        "Risk": risk,
        "Action": action,
        "Snippet": (r["Text"][:240] + "...") if len(r["Text"])>240 else r["Text"]
    })

df = pd.DataFrame(rows)
df.head(20)

# Filter rows where risk is not "None" or contains keywords
def is_critical(risk_text):
    if not risk_text or risk_text.strip().lower() == "none":
        return False
    low_keywords = ["minor","typographical","typo"]
    if any(k in risk_text.lower() for k in low_keywords):
        return False
    return True

critical_df = df[df["Risk"].apply(is_critical)].copy().reset_index(drop=True)
print("Critical risks found:", len(critical_df))
critical_df[["ClauseIndex","Snippet","Risk","Action"]]
